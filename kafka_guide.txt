============= Start ZooKeeper ===================
bin\windows\zookeeper-server-start.bat config\zookeeper.properties

============= Start Kafka ===================
bin\windows\kafka-server-start.bat config\server.properties

============= Create kafka topic ==============
bin\windows\kafka-topics.bat --create --topic your_topic_name --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1

============= Launch kafka producer ===============
bin\windows\kafka-console-producer.bat --topic bdccTopic --bootstrap-server localhost:9092

============== Launch kafka consumer ===================
bin\windows\kafka-console-consumer.bat --topic bdccTopic --bootstrap-server localhost:9092 --from-beginning


--------------------- SPARK ---------------------------------
spark-shell --packages org.apache.spark:spark-streaming-kafka-0-10_2.12:2.4.8


spark.stop

import org.apache.kafka.common.serialization.StringDeserializer
import org.apache.spark.SparkConf
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.streaming.kafka010._

// Configure the Kafka parameters
val kafkaParams = Map[String, Object](
  "bootstrap.servers" -> "localhost:9092", // Kafka broker addresses
  "key.deserializer" -> classOf[StringDeserializer],
  "value.deserializer" -> classOf[StringDeserializer],
  "group.id" -> "my-group", // Consumer group ID
  "auto.offset.reset" -> "latest", // Offset reset strategy
  "enable.auto.commit" -> (false: java.lang.Boolean) // Disable auto-commit
)

// Create a SparkConf and StreamingContext
val conf = new SparkConf().setAppName("KafkaConsumer").setMaster("local[*]")
val ssc = new StreamingContext(conf, Seconds(5))

// Define the topics to consume from
val topics = Array("bdccTopic")

// Create a DStream that represents the stream of messages from Kafka
val stream = KafkaUtils.createDirectStream[String, String](
  ssc,
  LocationStrategies.PreferConsistent,
  ConsumerStrategies.Subscribe[String, String](topics, kafkaParams)
)

// Display the messages
stream.foreachRDD { rdd =>
  rdd.foreach { record =>
    println(s"Key: ${record.key()}, Value: ${record.value()}")
  }
}

// Start the streaming context
ssc.start()
ssc.awaitTermination()



========================> Some processing operations <============================

spark.stop

import org.apache.kafka.common.serialization.StringDeserializer
import org.apache.spark.SparkConf
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.streaming.kafka010._

// Configure the Kafka parameters
val kafkaParams = Map[String, Object](
  "bootstrap.servers" -> "localhost:9092", // Kafka broker addresses
  "key.deserializer" -> classOf[StringDeserializer],
  "value.deserializer" -> classOf[StringDeserializer],
  "group.id" -> "my-group", // Consumer group ID
  "auto.offset.reset" -> "latest", // Offset reset strategy
  "enable.auto.commit" -> (false: java.lang.Boolean) // Disable auto-commit
)

// Create a SparkConf and StreamingContext
val conf = new SparkConf().setAppName("KafkaConsumer").setMaster("local[*]")
val ssc = new StreamingContext(conf, Seconds(5))

// Define the topics to consume from
val topics = Array("bdccTopic")

// Create a DStream that represents the stream of messages from Kafka
val stream = KafkaUtils.createDirectStream[String, String](
  ssc,
  LocationStrategies.PreferConsistent,
  ConsumerStrategies.Subscribe[String, String](topics, kafkaParams)
)

// Split and display the messages
stream.foreachRDD { rdd =>
  rdd.foreach { record =>
    val data = record.value()
    val fields = data.split('|')
    fields.foreach(println)
  }
}

// Start the streaming context
ssc.start()
ssc.awaitTermination()


=======================================>Cassandra<=======================================
----------------------- Start cassandra
bin\cassandra
------------------------ Start Shell
bin\cqlsh
---------------------------- Create KeySpace
CREATE KEYSPACE tashilat WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};
----------------------------- Connect to keyspace
USE my_keyspace;
----------------------------- Create Student table
CREATE TABLE student_table (
  cne text PRIMARY KEY,
  first_name text,
  last_name text,
  email text,
  gender text,
  phone text,
  address text,
  age int
);

==================================================>Spark and cassandra connection<=======================================
spark-shell --packages org.apache.spark:spark-streaming-kafka-0-10_2.12:2.4.8,com.datastax.spark:spark-cassandra-connector_2.12:3.1.0

spark.stop

import org.apache.kafka.common.serialization.StringDeserializer
import org.apache.spark.SparkConf
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.streaming.kafka010._
import com.datastax.oss.driver.api.core.CqlSession

// Configure the Kafka parameters
val kafkaParams = Map[String, Object](
  "bootstrap.servers" -> "localhost:9092", // Kafka broker addresses
  "key.deserializer" -> classOf[StringDeserializer],
  "value.deserializer" -> classOf[StringDeserializer],
  "group.id" -> "my-group", // Consumer group ID
  "auto.offset.reset" -> "latest", // Offset reset strategy
  "enable.auto.commit" -> (false: java.lang.Boolean) // Disable auto-commit
)

// Create a SparkConf and StreamingContext
val conf = new SparkConf().setAppName("KafkaConsumer").setMaster("local[*]")
val ssc = new StreamingContext(conf, Seconds(5))

// Define the topics to consume from
val topics = Array("bdccTopic")

// Create a DStream that represents the stream of messages from Kafka
val stream = KafkaUtils.createDirectStream[String, String](
  ssc,
  LocationStrategies.PreferConsistent,
  ConsumerStrategies.Subscribe[String, String](topics, kafkaParams)
)

// Split and save the messages to Cassandra
stream.foreachRDD { rdd =>
  rdd.foreachPartition { partition =>
    // Establish a new connection to Cassandra for each partition
    val cassandraSession = CqlSession.builder().build()

    partition.foreach { record =>
      val data = record.value()
      val fields = data.split('|')
      val lastName = fields(1) 
      val school_name = fields(9)  
      val firstName = fields(0) 
      val address = fields(5)  
      val cne = fields(7)  
      val phone = fields(4) 
      val cin = fields(6)  
      val gender = fields(3)  
      val ageStr = fields(8)
      val email= fields(2)  

      // Safely convert ageStr to integer using Try
      val age = scala.util.Try(ageStr.toInt).getOrElse(0)

      // Insert the data into Cassandra
      val insertQuery = s"""INSERT INTO tashilat.student_table (last_name, age, gender, school_name, cne, first_name, phone, address ,cin, email)
                           |VALUES ('$lastName', $age, '$gender', '$school_name', '$cne', '$firstName', '$phone', '$address', '$cin', '$email')""".stripMargin

      cassandraSession.execute(insertQuery)
    }

    // Close the Cassandra session for each partition
    cassandraSession.close()
  }
}

// Start the streaming context
ssc.start()
ssc.awaitTermination()







CREATE TABLE sensors.receiver_sensors_data (
  id UUID PRIMARY KEY,
  plane_name TEXT,
  altitude FLOAT,
  temperature FLOAT,
  fuel FLOAT,
  pressure FLOAT,
  speed FLOAT,
  oil_pressure FLOAT
);


==================================================>Spark and cassandra connection (sensors)<=======================================
spark-shell --packages org.apache.spark:spark-streaming-kafka-0-10_2.12:2.4.8,com.datastax.spark:spark-cassandra-connector_2.12:3.1.0

spark.stop

import org.apache.kafka.common.serialization.StringDeserializer
import org.apache.spark.SparkConf
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.streaming.kafka010._
import com.datastax.oss.driver.api.core.CqlSession
import com.datastax.oss.driver.api.core.uuid.Uuids

// Configure the Kafka parameters
val kafkaParams = Map[String, Object](
  "bootstrap.servers" -> "localhost:9092", // Kafka broker addresses
  "key.deserializer" -> classOf[StringDeserializer],
  "value.deserializer" -> classOf[StringDeserializer],
  "group.id" -> "my-group", // Consumer group ID
  "auto.offset.reset" -> "latest", // Offset reset strategy
  "enable.auto.commit" -> (false: java.lang.Boolean) // Disable auto-commit
)

// Create a SparkConf and StreamingContext
val conf = new SparkConf().setAppName("KafkaConsumer").setMaster("local[*]")
val ssc = new StreamingContext(conf, Seconds(5))

// Define the topics to consume from
val topics = Array("sensorsTopic")

// Create a DStream that represents the stream of messages from Kafka
val stream = KafkaUtils.createDirectStream[String, String](
  ssc,
  LocationStrategies.PreferConsistent,
  ConsumerStrategies.Subscribe[String, String](topics, kafkaParams)
)

// Process and save the messages to Cassandra
stream.foreachRDD { rdd =>
  rdd.foreachPartition { partition =>
    // Establish a new connection to Cassandra for each partition
    val cassandraSession = CqlSession.builder().build()

    partition.foreach { record =>
      val data = record.value()
      val fields = data.split('|')
      val id = Uuids.timeBased()
      val planeName = fields(0)
      val altitude = fields(1).toFloat
      val temperature = fields(2).toFloat
      val fuel = fields(3).toFloat
      val pressure = fields(4).toFloat
      val speed = fields(5).toFloat
      val oilPressure = fields(6).toFloat

      // Insert the data into Cassandra
      val insertQuery = s"""INSERT INTO sensors.receiver_sensors_data (id, plane_name, altitude, temperature, fuel, pressure, speed, oil_pressure)
                           |VALUES ($id, '$planeName', $altitude, $temperature, $fuel, $pressure, $speed, $oilPressure)""".stripMargin

      cassandraSession.execute(insertQuery)
    }

    // Close the Cassandra session for each partition
    cassandraSession.close()
  }
}

// Start the streaming context
ssc.start()
ssc.awaitTermination()






spark.stop

import org.apache.kafka.common.serialization.StringDeserializer
import org.apache.spark.SparkConf
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.streaming.kafka010._
import com.datastax.oss.driver.api.core.CqlSession
import com.datastax.oss.driver.api.core.uuid.Uuids
import java.time.Instant

// Configure the Kafka parameters
val kafkaParams = Map[String, Object](
  "bootstrap.servers" -> "localhost:9092", // Kafka broker addresses
  "key.deserializer" -> classOf[StringDeserializer],
  "value.deserializer" -> classOf[StringDeserializer],
  "group.id" -> "my-group", // Consumer group ID
  "auto.offset.reset" -> "latest", // Offset reset strategy
  "enable.auto.commit" -> (false: java.lang.Boolean) // Disable auto-commit
)

// Create a SparkConf and StreamingContext
val conf = new SparkConf().setAppName("KafkaConsumer").setMaster("local[*]")
val ssc = new StreamingContext(conf, Seconds(5))

// Define the topics to consume from
val topics = Array("sensorsTopic")

// Create a DStream that represents the stream of messages from Kafka
val stream = KafkaUtils.createDirectStream[String, String](
  ssc,
  LocationStrategies.PreferConsistent,
  ConsumerStrategies.Subscribe[String, String](topics, kafkaParams)
)

// Process and save the messages to Cassandra
stream.foreachRDD { rdd =>
  rdd.foreachPartition { partition =>
    // Establish a new connection to Cassandra for each partition
    val cassandraSession = CqlSession.builder().build()

    partition.foreach { record =>
      val data = record.value()
      val fields = data.split('|')
      val id = Uuids.timeBased()
      val planeName = fields(0)
      val altitude = fields(1).toFloat
      val temperature = fields(2).toFloat
      val fuel = fields(3).toFloat
      val pressure = fields(4).toFloat
      val speed = fields(5).toFloat
      val oilPressure = fields(6).toFloat
      val timestamp = Instant.now()

      // Insert the data into Cassandra
      val insertQuery = s"""INSERT INTO sensors.receiver_sensors_data (id, plane_name, altitude, temperature, fuel, pressure, speed, oil_pressure, timestamp)
                           |VALUES ($id, '$planeName', $altitude, $temperature, $fuel, $pressure, $speed, $oilPressure, '$timestamp')""".stripMargin

      cassandraSession.execute(insertQuery)
    }

    // Close the Cassandra session for each partition
    cassandraSession.close()
  }
}

// Start the streaming context
ssc.start()
ssc.awaitTermination()


Blue : #0f607e
Yellow : #bdca12
